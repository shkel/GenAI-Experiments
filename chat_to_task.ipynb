{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Automation of task creation for chat discussions\n\nBased on the results of the meeting or chat history, it is necessary to create tasks for developers.\nThe task consists of parts:\n- analyze the discussion and collect threads;\n- determine the type of thread (bug, feature or just discussion);\n- create a task (trello).\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"I will use Gemini. Install library and prepare notebook.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"\nfrom google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:57:35.036479Z","iopub.execute_input":"2025-04-19T09:57:35.037066Z","iopub.status.idle":"2025-04-19T09:57:46.600916Z","shell.execute_reply.started":"2025-04-19T09:57:35.03699Z","shell.execute_reply":"2025-04-19T09:57:46.599698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:57:46.602536Z","iopub.execute_input":"2025-04-19T09:57:46.603518Z","iopub.status.idle":"2025-04-19T09:57:46.875718Z","shell.execute_reply.started":"2025-04-19T09:57:46.603461Z","shell.execute_reply":"2025-04-19T09:57:46.874077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:57:46.878468Z","iopub.execute_input":"2025-04-19T09:57:46.879215Z","iopub.status.idle":"2025-04-19T09:57:47.324987Z","shell.execute_reply.started":"2025-04-19T09:57:46.879172Z","shell.execute_reply":"2025-04-19T09:57:47.323501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Understand text, split it into JSON","metadata":{}},{"cell_type":"markdown","source":"I'll use a text content, but it can be an audio/video (I mean text extraction).","metadata":{}},{"cell_type":"code","source":"# Input. The text to be analyzed.\npath_to_file = ''\n!wget path_to_file -O discussion.pdf\ndocument_file = client.files.upload(file='discussion.pdf')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:57:47.327258Z","iopub.execute_input":"2025-04-19T09:57:47.327677Z","iopub.status.idle":"2025-04-19T09:57:51.188029Z","shell.execute_reply.started":"2025-04-19T09:57:47.327646Z","shell.execute_reply":"2025-04-19T09:57:51.186717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We need a structured text as a result.\nSince the output length is limited with a large input file, it is possible to split the analysis into 2 steps: 1) find threads 2) in a loop for each thread find all the information. \nBut since I have a test version, we will do it all in one request.","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = '''The document contains a discussion between programmers and users about the application. Users mostly ask questions, suggest new features, and report bugs.\nYou need to select the important information from the chat and structure result into valid JSON.\nOne issue/suggestion/discussion should be in one thread. Each object can contain only one thread.\nThe user story must be full and contain all the details.\nThe subject should be short and reflect the problem or proposal.\n\nThe description is a technical summary needed to identify the type of problem (bug, feature request, question) written for developers. This is a text of the task for the developer. So use technical definitions here.\nIMPORTANT to understand whether this is a bug, a feature, or just a discussion and reflect this in the description.\nTypical words to describe a bug: error, crash, stack trace, OOM, etc.\nTypical words to describe a feature: suggest, add, create, would like, improve, etc.\n\nSeveral issues are discussed in the chat at the same time. You have to collect the threads of discussions yourself.\nIgnore irrelevant information (anything that is not related to the discussion of the application) or unclear context messages.\n\nEXAMPLE JSON Response:\n```\n{\n\"participants\" : [\"Jack\", \"Maria\"],\n\"subject\": \"Improve error message\",\n\"user_story\": \"When I press the X button, a message appears with a technical description. I don't understand what to do.\",\n\"description\" : \"Improve the error message. It must say what happend and what the user have to do. Fx, error loading data.\"\n}\n```\n\nEXAMPLE JSON Response:\n```\n{\n\"participants\" : [\"Daniel\"],\n\"subject\": \"The table is not displayed\",\n\"user_story\": \"I select the Tasks item in the menu. On the page, I select any task and click on the List of editors. A dialog opens in which the loader is displayed, which will never complete.\",\n\"description\" : \"Uncaught error or no data. We need to figure out what the problem is. And we need to show either the data or the error message.\"\n}\n```\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:33:19.209642Z","iopub.execute_input":"2025-04-19T10:33:19.210118Z","iopub.status.idle":"2025-04-19T10:33:19.215616Z","shell.execute_reply.started":"2025-04-19T10:33:19.210086Z","shell.execute_reply":"2025-04-19T10:33:19.214167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"At this stage, it is important to be creative and smart, so we need to use the latest model with high temperature.","metadata":{}},{"cell_type":"code","source":"# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300.0)\ndef summarise_doc(request: str) -> str:\n  \"\"\"Execute the request on the uploaded document.\"\"\"\n  config=types.GenerateContentConfig(\n        temperature=1.5,\n        # top_p=0.5,\n        response_mime_type=\"application/json\"\n  )\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=config,\n      contents=[request, document_file],\n  )\n\n  return response.text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:34:08.942751Z","iopub.execute_input":"2025-04-19T10:34:08.943177Z","iopub.status.idle":"2025-04-19T10:34:08.949754Z","shell.execute_reply.started":"2025-04-19T10:34:08.943143Z","shell.execute_reply":"2025-04-19T10:34:08.94841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = summarise_doc(few_shot_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:34:12.459942Z","iopub.execute_input":"2025-04-19T10:34:12.460367Z","iopub.status.idle":"2025-04-19T10:34:18.197384Z","shell.execute_reply.started":"2025-04-19T10:34:12.460336Z","shell.execute_reply":"2025-04-19T10:34:18.195487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# let's see the result\nimport json\nprint(json.dumps(json.loads(summary), indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T14:44:30.212228Z","iopub.execute_input":"2025-04-19T14:44:30.212833Z","iopub.status.idle":"2025-04-19T14:44:30.219959Z","shell.execute_reply.started":"2025-04-19T14:44:30.2128Z","shell.execute_reply":"2025-04-19T14:44:30.218163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classificator","metadata":{}},{"cell_type":"markdown","source":"Typical topics of daily developer discussions are bugs, features, or nice to have. These will be our classes.\nSince I don't have any public tasks for training the classifier, we'll generate them with a minimal description. High creativity and not too long content is required.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport json\n\ngen_config = types.GenerateContentConfig(max_output_tokens=250, temperature=2.0, response_mime_type=\"application/json\")\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300.0)\ndef generate_tasks(task_id : int, task_type : str, task_sample : str, max_cnt : int):\n  gen_prompt = f\"\"\"There is a task board for programmers working on Communication application (like Gmail, Google Chat, ...).\n                  IMPORTANT to create ONE task (one object) for the board in the column {task_type}. Think of any name for a person. Use markdown syntax for description. Escape all special symbols.\nThe description should be SHORT, specific and with technical details. That is a ready-made card for the performer. It can indicate the names of functions, places in the interface, stack trace, etc.\nDescription example : {task_sample}\n\nEXAMPLE Response:\n```\n[{{\n\"author\" : \"First_Name Last_Name\",\n\"subject\": \"Title\"\n\"description\": \"Description of the problem or suggestion.\"\n}}]\n```\n\"\"\"\n    \n  tasks = []\n  for _ in range(max_cnt):\n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        config=gen_config,\n        contents=gen_prompt\n    )\n    if response.text:\n        try:\n            obj_response = json.loads(response.text)\n            for item in obj_response:\n                item['class'] = task_id\n                item['type'] = task_type\n                tasks.append(item)\n        except json.JSONDecodeError:\n             # \"close\" json if generation stopped due to length\n            try:\n                normalized_response = response.text.removesuffix(']').removesuffix('\"}').removesuffix('\"\\n}') + '\"\\n}\\n]'\n                obj_response = json.loads(normalized_response)\n                for item in obj_response:\n                    item['class'] = task_id\n                    item['type'] = task_type\n                    tasks.append(item)\n            except json.JSONDecodeError:\n                print(normalized_response)\n\n  df_tasks = pd.DataFrame(tasks)\n  return df_tasks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:58:56.998188Z","iopub.execute_input":"2025-04-19T09:58:56.998667Z","iopub.status.idle":"2025-04-19T09:58:57.008497Z","shell.execute_reply.started":"2025-04-19T09:58:56.998631Z","shell.execute_reply":"2025-04-19T09:58:57.007207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate max 100 tasks for each type.","metadata":{"execution":{"iopub.status.busy":"2025-04-18T16:45:55.534611Z","iopub.execute_input":"2025-04-18T16:45:55.535028Z","iopub.status.idle":"2025-04-18T16:45:57.475169Z","shell.execute_reply.started":"2025-04-18T16:45:55.534998Z","shell.execute_reply":"2025-04-18T16:45:57.473887Z"}}},{"cell_type":"code","source":"bug_sample = \"\"\"The \"Address can not be empty\" error is shown on the map screen after selecting an address from google suggestions \\n # Steps to reproduce: 1) Tap the suggestion 2) Tap the green button to submit \\n Results Actual \\n The \"Address can not be empty\" error is shown on the map screen after selecting an address from google suggestions \\n Results Expected \\n Google suggestion is saved as the address to the \"Address\" field of the \"Profile info\" form after selecting an address from google suggestions\n\nUse words to describe the issue: error, crash, stack trace, OOM, etc.\n\"\"\"\nbugs = generate_tasks(1, 'BUG', bug_sample, 100)\n\nfeature_sample = \"\"\"Description of NEED: User profiles pictures don’t have any personalization options, like the ability to draw mustaches or smiles on one’s profile picture.  \\n In the Account Settings page, ADD “Draw on photo” button. Clicking on that opens up a simple line drawing tool and color picker like the one you see in Preview app.\n\nUse words to describe the feature: suggest, add, need, create, want, etc.\"\"\"\nfeatures = generate_tasks(2, 'New Feature', feature_sample, 100)\n\nbad_sample = '''I don't understand where to click to upload my picture. \\n I can't find the user in my contact list. \\n Is it possible to download my certificate?\n\nUsually there is NO specific suggestion or complaint.\n'''\nunrecognized = generate_tasks(3, 'Unsorted', bad_sample, 100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T09:58:57.010289Z","iopub.execute_input":"2025-04-19T09:58:57.010751Z","iopub.status.idle":"2025-04-19T10:18:03.949366Z","shell.execute_reply.started":"2025-04-19T09:58:57.010707Z","shell.execute_reply":"2025-04-19T10:18:03.947825Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split generated data into 2 datasets : train and test.","metadata":{}},{"cell_type":"code","source":"df_train = pd.concat([bugs.iloc[:70], features.iloc[:70], unrecognized.iloc[:70]]).reset_index(drop=True)\ndf_test = pd.concat([bugs.iloc[-29:], features.iloc[-29:], unrecognized.iloc[-29:]]).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:18:03.95224Z","iopub.execute_input":"2025-04-19T10:18:03.952689Z","iopub.status.idle":"2025-04-19T10:18:03.964274Z","shell.execute_reply.started":"2025-04-19T10:18:03.952653Z","shell.execute_reply":"2025-04-19T10:18:03.962651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create embeddings for generated datasets","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nimport tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Add tqdm to Pandas...\ntqdmr.pandas()\n\n# ...But suppress the experimental warning.\nwarnings.filterwarnings(\"ignore\", category=tqdm.TqdmExperimentalWarning)\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n@retry.Retry(predicate=is_retriable, timeout=300.0)\ndef embed_fn(text: str) -> list[float]:\n    # You will be performing classification, so set task_type accordingly.\n    response = client.models.embed_content(\n        model=\"models/text-embedding-004\",\n        contents=text,\n        config=types.EmbedContentConfig(\n            task_type=\"classification\",\n        ),\n    )\n\n    return response.embeddings[0].values\n\n\ndef create_embeddings(df):\n    df[\"text\"] = df[\"subject\"] + df[\"description\"]\n    df[\"Embeddings\"] = df[\"text\"].progress_apply(embed_fn)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:18:03.966035Z","iopub.execute_input":"2025-04-19T10:18:03.966381Z","iopub.status.idle":"2025-04-19T10:18:03.995123Z","shell.execute_reply.started":"2025-04-19T10:18:03.966348Z","shell.execute_reply":"2025-04-19T10:18:03.993076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_e_train = create_embeddings(df_train)\ndf_e_test = create_embeddings(df_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:18:03.996685Z","iopub.execute_input":"2025-04-19T10:18:03.997092Z","iopub.status.idle":"2025-04-19T10:20:41.647978Z","shell.execute_reply.started":"2025-04-19T10:18:03.99706Z","shell.execute_reply":"2025-04-19T10:20:41.646184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# let's check the result\ndf_e_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:20:41.649635Z","iopub.execute_input":"2025-04-19T10:20:41.650092Z","iopub.status.idle":"2025-04-19T10:20:41.671753Z","shell.execute_reply.started":"2025-04-19T10:20:41.650052Z","shell.execute_reply":"2025-04-19T10:20:41.670329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Make Classificator","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras import layers\n\n\ndef build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n    return keras.Sequential(\n        [\n            layers.Input([input_size], name=\"embedding_inputs\"),\n            layers.Dense(input_size, activation=\"relu\", name=\"hidden\"),\n            layers.Dense(num_classes, activation=\"softmax\", name=\"output_probs\"),\n        ]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:20:41.673569Z","iopub.execute_input":"2025-04-19T10:20:41.674034Z","iopub.status.idle":"2025-04-19T10:20:58.935913Z","shell.execute_reply.started":"2025-04-19T10:20:41.673965Z","shell.execute_reply":"2025-04-19T10:20:58.934931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Derive the embedding size from observing the data. The embedding size can also be specified\n# with the `output_dimensionality` parameter to `embed_content` if you need to reduce it.\nembedding_size = len(df_e_train[\"Embeddings\"].iloc[0])\n\nclassifier = build_classification_model(\n    embedding_size, len(df_e_train[\"type\"].unique())\n)\nclassifier.summary()\n\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(),\n    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n    metrics=[\"accuracy\"],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:20:58.938588Z","iopub.execute_input":"2025-04-19T10:20:58.939223Z","iopub.status.idle":"2025-04-19T10:20:59.067443Z","shell.execute_reply.started":"2025-04-19T10:20:58.939192Z","shell.execute_reply":"2025-04-19T10:20:59.065887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n\nNUM_EPOCHS = 20\nBATCH_SIZE = 32\n\n# Split the x and y components of the train and validation subsets.\ny_train = df_e_train[\"class\"].transform(lambda x: x - 1)\nx_train = np.stack(df_e_train[\"Embeddings\"])\ny_val = df_e_test[\"class\"].transform(lambda x: x - 1)\nx_val = np.stack(df_e_test[\"Embeddings\"])\n\n# Specify that it's OK to stop early if accuracy stabilises.\nearly_stop = keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n\n# Train the model for the desired number of epochs.\nhistory = classifier.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_val, y_val),\n    callbacks=[early_stop],\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:20:59.06875Z","iopub.execute_input":"2025-04-19T10:20:59.069071Z","iopub.status.idle":"2025-04-19T10:21:02.458571Z","shell.execute_reply.started":"2025-04-19T10:20:59.069044Z","shell.execute_reply":"2025-04-19T10:21:02.457174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifier.evaluate(x=x_val, y=y_val, return_dict=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:21:02.459917Z","iopub.execute_input":"2025-04-19T10:21:02.460288Z","iopub.status.idle":"2025-04-19T10:21:02.559945Z","shell.execute_reply.started":"2025-04-19T10:21:02.460259Z","shell.execute_reply":"2025-04-19T10:21:02.5587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classify initial data","metadata":{}},{"cell_type":"markdown","source":"Calculate the probability of classifying our text into a specific class.","metadata":{}},{"cell_type":"code","source":"def make_prediction(text: str) -> list[float]:\n    \"\"\"Infer categories from the provided text.\"\"\"\n    # Remember that the model takes embeddings as input, so calculate them first.\n    embedded = embed_fn(text)\n\n    # And recall that the input must be batched, so here they are wrapped as a\n    # list to provide a batch of 1.\n    inp = np.array([embedded])\n\n    # And un-batched here.\n    [result] = classifier.predict(inp)\n    result[::-1].sort()\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:21:02.561125Z","iopub.execute_input":"2025-04-19T10:21:02.561512Z","iopub.status.idle":"2025-04-19T10:21:02.567073Z","shell.execute_reply.started":"2025-04-19T10:21:02.561481Z","shell.execute_reply":"2025-04-19T10:21:02.565676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"issues_from_chat = json.loads(summary)\ndf_issues = pd.DataFrame(issues_from_chat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:37:24.158374Z","iopub.execute_input":"2025-04-19T10:37:24.158834Z","iopub.status.idle":"2025-04-19T10:37:24.164775Z","shell.execute_reply.started":"2025-04-19T10:37:24.158802Z","shell.execute_reply":"2025-04-19T10:37:24.163467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, row in df_issues.iterrows():\n    print(index, row['description'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T10:37:33.754978Z","iopub.execute_input":"2025-04-19T10:37:33.755392Z","iopub.status.idle":"2025-04-19T10:37:33.765323Z","shell.execute_reply.started":"2025-04-19T10:37:33.755361Z","shell.execute_reply":"2025-04-19T10:37:33.76283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_prediction(df) :\n    for index, row in df.iterrows():\n        result = make_prediction(row['subject'] + row['description'])\n        for idx, category in enumerate(df_test[\"type\"].astype(\"category\").cat.categories):\n            df.at[index, category.lower()] = f\"{result[idx] * 100:0.2f}\"\n    return df\n    \n\ndf_issue_cat = add_prediction(df_issues)\n\ndf_issue_cat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T11:12:18.577093Z","iopub.execute_input":"2025-04-19T11:12:18.577694Z","iopub.status.idle":"2025-04-19T11:12:32.823379Z","shell.execute_reply.started":"2025-04-19T11:12:18.57766Z","shell.execute_reply":"2025-04-19T11:12:32.822289Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create cards","metadata":{}},{"cell_type":"markdown","source":"We have pre-processed all the data. Now we will use Gemini to create cards. \nTo do this, we will create a couple of functions (with HTTP requests inside) and ask the AI to analyze the data, what the user wants, and perform the necessary actions.","metadata":{}},{"cell_type":"code","source":"# these values should be taken from the environment\nTRELLO_API_KEY = ''\nTRELLO_TOKEN = ''\nboard_id = ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:02:57.964061Z","iopub.execute_input":"2025-04-19T15:02:57.964416Z","iopub.status.idle":"2025-04-19T15:02:57.969359Z","shell.execute_reply.started":"2025-04-19T15:02:57.96439Z","shell.execute_reply":"2025-04-19T15:02:57.96792Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Helpers","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\n\n# return a link\ndef create_trello_card(text : str, list_id : str) -> str:\n    card = json.loads(text)\n    if (list_id == ''):\n        return ''\n    else :\n        url = \"https://api.trello.com/1/cards\"\n        headers = {\n          \"Accept\": \"application/json\"\n        }\n        query = {\n            'key': TRELLO_API_KEY,\n            'token': TRELLO_TOKEN,\n            'idList' : list_id,\n            'name' : card['subject'],\n            'desc' : card['description'] + '\\n ___________________ \\nParticipants: ' + ' '.join(card['participants']) + '\\n' + card['user_story']\n        }\n        response = requests.request(\n           \"POST\",\n           url,\n           headers=headers,\n           params=query\n        )\n        try:\n            url = json.dumps(json.loads(response.text)['url'])\n            return url\n        except json.JSONDecodeError:\n            return ''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:03:01.240136Z","iopub.execute_input":"2025-04-19T15:03:01.240514Z","iopub.status.idle":"2025-04-19T15:03:01.247789Z","shell.execute_reply.started":"2025-04-19T15:03:01.240483Z","shell.execute_reply":"2025-04-19T15:03:01.246274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\n# response is a dictionary of lists (name and id)\ndef get_trello_lists() -> list[tuple[str]]:\n    url = f\"https://api.trello.com/1/boards/{board_id}/lists\"\n    \n    query = {\n      'key': TRELLO_API_KEY,\n      'token': TRELLO_TOKEN\n    }\n    \n    response = requests.request(\n       \"GET\",\n       url,\n       headers=headers,\n       params=query\n    )\n    \n    try:\n        board_lists = []\n        for item in json.loads(response.text):\n            board_list = (item['name'] , item['id'])\n            board_lists.append(board_list)\n        return board_lists\n    except json.JSONDecodeError:\n        print(response)\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:03:05.395286Z","iopub.execute_input":"2025-04-19T15:03:05.395688Z","iopub.status.idle":"2025-04-19T15:03:05.401873Z","shell.execute_reply.started":"2025-04-19T15:03:05.395656Z","shell.execute_reply":"2025-04-19T15:03:05.400626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Define the main function for AI.","metadata":{}},{"cell_type":"code","source":"def to_card(discussion : str, user_prompt : str) -> str :\n    request_tools = [get_trello_lists, create_trello_card]\n    \n    instruction = f\"\"\"You are a helpful chatbot that can create a Trello card create cards based on discussions of user issues in chat.\n    You will analyze the users request and turn them into commands using the tools available. Once you have the information you need, you will\n    answer the user's question using the data returned.\n    \n    Use get_trello_lists to find available lists, response is a tuple (name, ID).\n    After that analyze the provided TEXT to understand in which column the card can be created. If you don't need to create a card, ID should be an empty string. The last columns contain the probability of assigning a problem to a type (list).\n    If you decide to create a card, then create a card in the appropriate column using create_trello_card. You must use the entire TEXT given to you as an argument to the function WITHOUT modification. Return an extracted url of the created card from response.\n\n    You must return either a string with a link to the created card, or write why the card was not created.\n    \n    TEXT : {discussion}\n    \"\"\"\n    \n    # Start a chat with automatic function calling enabled.\n    trello_chat = client.chats.create(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=instruction,\n            tools=request_tools,\n        ),\n    )\n    \n    response = trello_chat.send_message(user_prompt)\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:05:45.483348Z","iopub.execute_input":"2025-04-19T15:05:45.48385Z","iopub.status.idle":"2025-04-19T15:05:45.491045Z","shell.execute_reply.started":"2025-04-19T15:05:45.483819Z","shell.execute_reply":"2025-04-19T15:05:45.489632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_prompt = 'Create a card only if there is a CRITICAL bug being discussed.'\nfor i in df_issue_cat.index:\n    description = df_issue_cat.loc[i].to_json()\n    link = to_card(description, user_prompt).text\n    print(str(i + 1), link)\n    print(json.dumps(json.loads(description)['subject'], indent=2))\n    print('-----------------')\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:06:27.061366Z","iopub.execute_input":"2025-04-19T15:06:27.061695Z","iopub.status.idle":"2025-04-19T15:07:20.879777Z","shell.execute_reply.started":"2025-04-19T15:06:27.06167Z","shell.execute_reply":"2025-04-19T15:07:20.878464Z"}},"outputs":[],"execution_count":null}]}